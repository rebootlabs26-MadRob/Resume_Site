
Local LLM Management + Terminal Efficiency

ðŸ”¹ Model Management

â€¢ `ollama list` â€” list installed models  
â€¢ `ollama pull` â€” download model  
â€¢ `ollama rm` â€” remove model  
â€¢ `ollama show` â€” show metadata

ðŸ”¹ Running Models

â€¢ `ollama run` â€” interactive chat  
â€¢ `ollama run --prompt "` â€” one-shot prompt  
â€¢ `ollama run -f` â€” run prompt from file

ðŸ”¹ Model Creation / Modding

â€¢ `ollama create -f Modelfile`  
â€¢ `ollama serve` â€” start Ollama server  
â€¢ `ollama pull :latest` â€” update

ðŸ”¹ Networking / API

â€¢ `curl http://localhost:11434/api/generate -d '{ "model": "", "prompt": "`  
â€¢ `ollama ps` â€” list running sessions

ðŸ”¹ Troubleshooting

â€¢ Model wonâ€™t run â†’ `ollama serve`  
â€¢ Port conflict â†’ kill process using port **11434**  
â€¢ Model corrupted â†’ `ollama rm` then reâ€‘pull  
â€¢ Slow performance â†’ check GPU drivers

	To Use/Talk 
	olla "...."

